{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38d572b",
   "metadata": {},
   "source": [
    "## Some fundamental elements of programming III\n",
    "### Understanding and creating correlated datasets and how to create functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809aa3e2",
   "metadata": {},
   "source": [
    "As we said before, the core of data science is computer programming. \n",
    "\n",
    "To really explore data, we need to be able to write code to \n",
    "\n",
    "  (1) wrangle or even generate data that has the properties needed for analysis and \n",
    "  \n",
    "  (2) do actual data analysis and visualization.\n",
    "\n",
    "If data science didn't involve programming – if it only involved clicking buttons in a statistics program like SPSS – it wouldn't be called data *science*. In fact, it wouldn't even be a \"thing\" at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30775168",
   "metadata": {},
   "source": [
    "Learning goals:\n",
    " \n",
    " - Understand how to generate correlated variables.\n",
    " \n",
    " - Understand function definitions in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc85df",
   "metadata": {},
   "source": [
    "#### Generate correlated datasets\n",
    "\n",
    "In thispart of the tutorial we will learn how generate datasets that are 'related.' While doing that we will practice a few things learned recently in previous tutorials:\n",
    "\n",
    "  - Plotting with matplotlib\n",
    "  - generating numpy arrays\n",
    "  - indexing into arrays\n",
    "  - using `while` loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d921de",
   "metadata": {},
   "source": [
    "First thing first, we will import the basic libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ca534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae26e00",
   "metadata": {},
   "source": [
    "After that we will create a few datasets. More specifically, we will create `n` datasets each called `x` (say 5 datasets, where `n=5`). Each dataset will have the length of `m` (where for example, `m` could be 100), this means for example that each dataset will have the shape of (m,1) or in our example (100,1).\n",
    "\n",
    "After that, we will create another group of `n` datasets called `y` of the same shape of `x`. Each one of the `y` datasets will have a corresponding `x` dataset that it will be correlated with.\n",
    "\n",
    "This means that for each dataset in `x` there will be a dataset in `y` that is correlated with it.\n",
    "\n",
    "Let's get started with a hands on method. Forst we will make the example of a single dataset `x` and a correlated dataset `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90569e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first build the dataset `x`, \n",
    "# we will use our standard method\n",
    "# based on randn\n",
    "m  = 1000;\n",
    "mu = 5;\n",
    "sd = 1;\n",
    "x  = np.random.randn(m,1);\n",
    "\n",
    "# let take a look at it\n",
    "plt.hist(x, 60);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee412a",
   "metadata": {},
   "source": [
    "OK. After generating the first dataset we will generate a second dataset, let's call it `y`. This second dataset will be correlated to the first.\n",
    "\n",
    "To generate a dataset correlated to `x` we will indeed use `x` as our base for the data and add on top of `x` a small amount of noise, let's call it `noise`. `noise` represents the small (or larger) difference between `x` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.random.randn(m,1);\n",
    "y = x + err\n",
    "plt.hist(y,60);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035d734",
   "metadata": {},
   "source": [
    "OK. The two histograms seem similar (similar range and height), but it is difficult to judge if `x` and `y` are indeed correlated. To do that we need to make a scatter plot.\n",
    "\n",
    "`matplotlib` has a convenient function for scatter plots, `plt.scatter()`, we will use that function to take a look at whether the two datasets are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aac1a9",
   "metadata": {},
   "source": [
    "Great, the symbols should be aligned along the major diagonal. This means that they are indeed correlated. To get to understand more what we did above, let's think about `err`.\n",
    "\n",
    "Imagine, if there were no error, e.g., no `err`. That would mean that there would be no difference between `x` and `y`. Literally, the two datasets would be identical.\n",
    "\n",
    "We can do that with the code above by setting `err` to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbe9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = 0\n",
    "y = x + err\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861e071",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "932919e2",
   "metadata": {},
   "source": [
    "The symbols should all lay on the major diagonal. So, `err` effectively controls the level of correlation between `x` and `y`. So if we set it to something small, in other words if we add only a small amount of error then the two arrays (`x` and `y`) would be very similar. For example, let's try setting it up to 10% of the original `err`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ce0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.random.randn(m,1);\n",
    "err = err*0.1 # 0.1 -> scaling factor\n",
    "y = x + err\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c6e9b",
   "metadata": {},
   "source": [
    "OK. It should have worked. The error added is not large, the symbols should lay almost on the diagonal, but not quite.\n",
    "\n",
    "As we increase the `err` the symbols should move away from the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.random.randn(m,1);\n",
    "scaling_factor = 0.9\n",
    "err = err*scaling_factor \n",
    "y = x + err\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c2e2c",
   "metadata": {},
   "source": [
    "One way to think about the scaling factor and `err` is that they are related to correlation. Indeed, they are not directly related to correlation. \n",
    "\n",
    "The scaling factor is inversely related to correlation because as the scaling factor increases the correlation decreases. Furthermore, they are not directly related to correlation because they both depend on a couple of variables, for example, the variance of the distributions (both `err` and `x` will affect the relationship between the correlation and the scaling factor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d49cd6",
   "metadata": {},
   "source": [
    "Python has a method to generate couples of correlated arrays. We will now briefly explore it, but leave a deeper dive on each function to you. You are suggested to further explore the code below and its implications. It might come helpful to us later down the road, you never know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d7803",
   "metadata": {},
   "source": [
    "#### A more principled way to make correlated datasets\n",
    "\n",
    "NumPy has a function called `multivariate_normal` that generates pairs of correlated datasets. The correlation values can be specified conveniently. A little bit of thinking is required, though. The function uses the covariance matrix. The covariance matrix is composed of 4 numbers. Two of the numbers describe the variances of the two datasamples we want to generate. The other two values describe the correlation between the samples and are generally called `covariances` (co-variations or co-relations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa555a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  numpy.random import multivariate_normal # we import the function\n",
    "x_mu = 0; # we set up the mean of the first set of data points \n",
    "y_mu = 0; # we set up the mean of the second sample\n",
    "x_var = 1; # the variance of the first sample\n",
    "y_var = 1; # the variance of the second sample\n",
    "cov = 0.9; # this is the covariance (can be thought of as correlation)\n",
    "\n",
    "# the function multivariate_normal will need a matrix to control\n",
    "# the relation between the samples, this matrix is called covariance matrix\n",
    "cov_m = [[x_var, cov],\n",
    "         [cov, y_var]]\n",
    "\n",
    "# we now create the two data sets by setting the the proper\n",
    "# means and passing the covariance matrix, we also pass the\n",
    "# requested size of the sample\n",
    "data = multivariate_normal([x_mu, y_mu], cov_m, size=1000)\n",
    "\n",
    "# We can plot the two data sets\n",
    "x, y = data[:,0], data[:,1]\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec207a",
   "metadata": {},
   "source": [
    "#### Creating many correlated datasets\n",
    "\n",
    "Imagine now if we were asked to create a series of correlated datasets. Not one, nottwo, more than that.\n",
    "\n",
    "Once the basic code used to build one is known. The rest of the datasets can be generated reusing the same code and putting the code inside a loop. Below we will show how to create 5 datasets using a `while` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c20772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0;\n",
    "n_datasets = 5;\n",
    "siz_datasets = 1000;\n",
    "\n",
    "x_mu = 1;  # mean of the first dataset \n",
    "y_mu = 1;  # mean of the second dataset\n",
    "x_var = 2; # the variance of the first dataset\n",
    "y_var = 2; # the variance of the second dataset\n",
    "cov = 0.85; # this is the covariance (can be thought of as correlation)\n",
    "\n",
    "# covariance matrix\n",
    "cov_m = [[x_var, cov],\n",
    "         [cov, y_var]]\n",
    "\n",
    "while counter < n_datasets :\n",
    "    data = multivariate_normal([x_mu, y_mu], \n",
    "                               cov_m, \n",
    "                               size=siz_datasets)\n",
    "    x, y = data[:,0], data[:,1]\n",
    "    counter = counter + 1\n",
    "\n",
    "    # Make a plot, show it, wait some time\n",
    "    print(\"Plotting dataset: \", counter)\n",
    "    plt.scatter(x, y);\n",
    "    plt.show() ;\n",
    "    plt.pause(0.05)\n",
    "\n",
    "else:\n",
    "    print(\"DONE Plotting datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31049f88",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Imagine if you were taking a class where you were asked often to create:\n",
    "\n",
    "  - Datasets of different means and standard deviations, say using code like: \n",
    "     `data = mu + sd*np.random.randn(siz,1);` \n",
    "     We have used this code before.\n",
    "  - Histograms of the datasets. For example using `sns.kdeplot`.\n",
    "  \n",
    "You might be tempted to be a very diligent student and memorize the code. That would be fine. But imagine being a pro data scientists, handling multiple 100s of requests from multiple managers and clients. Many of these requests will require rewriting, reusing the same block of code. Given that your day at work is likely to end after you respond to all the requests, methods to speed up your typing, reduce the amount of typing or requde the mistakes in typing would all be effective in making sure your day at work ends before sunset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdda51",
   "metadata": {},
   "source": [
    "**Functions** are often used to address situations just like the ones described above.\n",
    "\n",
    "[Functions, a.k.a. subroutines](https://en.wikipedia.org/wiki/Subroutine), are sequences of code packaged as units. The units can be called at different locations in code to quickly and efficiently perform the oprations implemented in the packaged code.\n",
    "\n",
    "Functions accept inputs and return outputs. Functions, contain useful code that is likely to be used multiple times. Functions can make the code more nimble, and can help memory (as do not require to rewrite the code).\n",
    "\n",
    "In python functions are defined with a function name and the special word `def` in front. Let's make a first example of a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402a675",
   "metadata": {},
   "source": [
    "Imagine wanting to make a function that returns the equivalent of a coin flip. The code below will do that for us. Let's analyze this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8caae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coinflip() :\n",
    "    import numpy as np\n",
    "    flip = np.random.randint(2) # return a random 0 or 1\n",
    "    if flip == 0 :\n",
    "        result = 'Tails!'\n",
    "    else :\n",
    "        result = 'Heads!'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02032eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test this function. \n",
    "# Run this cell multiple times\n",
    "coinflip()\n",
    "\n",
    "# You should get Tails or Heads!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfef2ea",
   "metadata": {},
   "source": [
    "### Anatomy of a function\n",
    "\n",
    "Let's analyze the function above. The first line of a function is the start. It contains a keyword that announces its definition `def` followed by the name of the function `coinfli()`\n",
    "\n",
    "`def coinflip()`\n",
    "\n",
    "The code inside the function is a module, isolated from the outside world. It is not affected by the code outside of the function except if it has inouts (out simple case does not have inputs, we will discuss that in a later tutorial).\n",
    "\n",
    "Because the code is isolated, we need to make sure we do all the imports needed and define all the variables needed **inside the function**. Imports done outside of the function. Variables definitions done outside of the function  are different than one another. Even those with the same names, say an x var defined inside the function and one defined outside of the function are not the same.\n",
    "\n",
    "So the second line above imports numpy as we will call numpy functions (\"hey!\") inside our function: `import numpy as np`\n",
    "\n",
    "The following lines are standard code that coule live inside as well as outside of this function.\n",
    "```\n",
    "flip = np.random.randint(2) # return a random 0 or 1\n",
    "    if flip == 0 :\n",
    "        result = 'Tails!'\n",
    "    else :\n",
    "        result = 'Heads!'\n",
    "```\n",
    "\n",
    "Yet, the final line of our function is a special one. It express the variable that needs to be returned to the rest fo the code, outside of the function.\n",
    "\n",
    "`return result`\n",
    "\n",
    "The variable `result` is the only variable that communicates outside of this function. The variable is generated inside `coinflip()` and its value is returned outside of `coinflip()` for the rest of the code to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9eb656",
   "metadata": {},
   "source": [
    "#### Another example function\n",
    "\n",
    "Let's go back to one of our needs. Imagine wanting to use what you just learned about functions to write one that generates distributions of normally distributed random numbers with a certain standard deviation.\n",
    "\n",
    "The code we have been using is the following: `data = mu + sd*np.random.randn(siz,1);`\n",
    "\n",
    "Let's make a function out of that so to not have to rewrite the same lines over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_data(mu,sd,siz):\n",
    "    import numpy as np\n",
    "    data = mu + sd*np.random.randn(siz,1);\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data(5,2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462615ad",
   "metadata": {},
   "source": [
    "OK. It works. The function, imports numpy, then calls randn to generate the data and then returns the data. That is pretty similar to the previous example.\n",
    "\n",
    "But, this new function, has inputs. Yes, the code needs to receive inputs from the code outside the function. These inputs define the mean ofthe distribution (`mu`), the standard deviation (`sd`), and the size of the dataset (`siz`).\n",
    "\n",
    "Functions, can take inputs and the type of inputs, the quantity and type of input is ll defined when the function is created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7a511",
   "metadata": {},
   "source": [
    "We will learn more about functions in the exercise and in future tutorials. Below, try to write a function that generates correlated datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2f7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
